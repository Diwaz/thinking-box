import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { tool } from "@langchain/core/tools";
import * as z from "zod";
import { StateGraph, START, END, Command } from "@langchain/langgraph";
import { MessagesZodMeta } from "@langchain/langgraph";
import { registry } from "@langchain/langgraph/zod";
import { type BaseMessage } from "@langchain/core/messages";
import { isAIMessage, ToolMessage } from "@langchain/core/messages";
import { SystemMessage } from "@langchain/core/messages";
import { HumanMessage } from "@langchain/core/messages";
import {exec} from "child_process"
import { applyPatch, createPatch } from "diff";
import { Sandbox } from '@e2b/code-interpreter'
import { SYSTEM_PROMPT } from "./prompt";
import express from 'express';
import cors from 'cors';
import { WebSocket, WebSocketServer } from "ws";


const app = express();

app.use(express.json());
app.use(cors());

interface ConversationHistory {
  messages :[]
}

const llm = new ChatGoogleGenerativeAI({
  model: "gemini-2.5-flash-lite",
});


const sandbox = await Sandbox.create("zcxo3nr01udjamtzcdn4")

const host = sandbox.getHost(5173)
console.log(`https://${host}`)

const createfile = tool(
  async ({ filePath, content }) => {
    const file = Bun.file(filePath);
    console.log("paths to write",file)
    // await Bun.write(file, content);

    await sandbox.files.write(
  filePath,
  content,
)
    return `File created successfully at ${filePath}`
  }, {
  name: "creates_new_file",
  description: "Creates new file and adds content to it",
  schema: z.object({
    filePath: z.string().describe("File path of the origin of the file"),
    content: z.string().describe("content or code to put inside file")
  })
})

const runShellCommands = tool (
  async ({ command }) => {
    console.log("cmd:",command);
    await sandbox.commands.run(
  `${command}`, {
  onStdout: (data) => {
    console.log("command output e2b:",data)
  },
  onStderr: (data) => {
    console.log("command error e2b:",data)
  },
}
    );


//     exec(`${command}`, (error, stdout, stderr) => {
//     if (error) {
//         console.log(`error: ${error.message}`);
//         return;
//     }
//     if (stderr) {
//         console.log(`stderr: ${stderr}`);
//         return;
//     }
//     console.log(`stdout: ${stdout}`);
// });

    return ` command executed in the terminal successfully `;
  },{
    name : "run_shell_command",
    description:"runs the shell command given by AI in the terminal",
    schema: z.object ({
      command: z.string().describe("shell command to run in bash terminal")
    })
  }
)
// const applyPatchTool = tool (
//   async ({file_to_edit,new_file})=>{
//     console.log("applying patch")
//     // apply patch logic 
//     console.log("file path of the old file",file_to_edit)
//     const oldFile = Bun.file(file_to_edit)
//     const oldFilesContents = await oldFile.text();
//     const patch = createPatch(file_to_edit,oldFilesContents,new_file)
//     const patchedFile = applyPatch(oldFilesContents,patch)
//     console.log("after applying patch",patchedFile)
//     await sandbox.files.write(
//   file_to_edit,
//   patchedFile,
// )
//     // await Bun.write(file_to_edit,patchedFile)
//   },
// {
//   name: "apply_patch_tool",
//   description: "takes the file path of the old file to edit and new content to apply patch and updates the old file with updated content",
//   schema: z.object({
//     file_to_edit: z.string().describe("file path of the old file to edit new update generated by AI"),
//     new_file: z.string().describe("new updated latest file given by the AI"),
//   })
// }
// )


const toolsByName = {
  [createfile.name]: createfile,
  [runShellCommands.name]: runShellCommands,
  // [applyPatchTool.name]:applyPatchTool
};

const tools = Object.values(toolsByName);
const llmWithTools = llm.bindTools(tools);


const MessageState = z.object({
  messages: z.array(z.custom<BaseMessage>()).register(registry, MessagesZodMeta),
  llmCalls: z.number().optional()
})

type State = z.infer<typeof MessageState>;

async function llmCall(state: State) {

  // if (state.llmCalls == 0){
    console.log("1st LLM CALLLLLLLL")
  const llmResponse = await llmWithTools.invoke([
    new SystemMessage(SYSTEM_PROMPT),
    ...state.messages
  ])

  const newCallCount = state.llmCalls + 1
  console.log("state of llmCalls",state.llmCalls)
  return {
    messages: [...state.messages, llmResponse],
    llmCalls: newCallCount,
  }


}

async function toolNode(state: State) {
  const lastMessage = state.messages.at(-1);

  if (lastMessage == null || !isAIMessage(lastMessage)) {
    return {
      messages: [],
    }
  }

  const result: ToolMessage[] = [];
  for (const toolCall of lastMessage.tool_calls ?? []) {
    const tool = toolsByName[toolCall.name];
    if (!tool) continue;
    const observation = await tool.invoke(toolCall);
    result.push(
      new ToolMessage({
        tool_call_id: toolCall.id,
        content:observation
      })
    );

  }

  return {
    messages: result
  }

}
async function shouldContinue(state: State) {
  const lastMessage = state.messages.at(-1);
  if (lastMessage == null || !isAIMessage(lastMessage)) {
    return END
  }

  if (lastMessage.tool_calls?.length) {
    return "toolNode";
  }
  return END;
}

const agent = new StateGraph(MessageState)
  .addNode("llmCall", llmCall)
  .addNode("toolNode", toolNode)
  .addEdge(START, "llmCall")
  .addConditionalEdges("llmCall", shouldContinue, ["toolNode", END])
  .addEdge("toolNode", "llmCall")
  .compile();

// const start_agent = async () => {
//   let state:State = {
//     messages: [],
//     llmCalls:0,
//   }
//   while (true) {
//     let inputFromUser = prompt("[You]:")

//     if (inputFromUser?.toLowerCase() === "quit"){
//       break;
//     }
//     if (!inputFromUser?.trim()){
//       continue
//     }
//     state.messages.push(new HumanMessage(inputFromUser))
//     const result = await agent.invoke(state)
//     state = result

//     for (const messages of state.messages) {
//       console.log(`[${messages._getType()}]:${messages.content}`);

//     }




//   }



// }
// start_agent();

const state:State ={
  messages:[],
  llmCalls:0,
}

app.post("/prompt",async (req,res)=>{
  const {prompt,projectId} = req.body;
  console.log("reached here w/ prompt",prompt)
  state.messages.push(new HumanMessage(prompt))

  const result = await agent.invoke(state)

  res.status(200).json({
   url:host,
    messages:result.messages,
  })

})

const ws = new WebSocketServer({port:8585});
ws.on('connection',(wss)=>{
  console.log("user connected")
wss.on('message',(data)=>{
  console.log("data from ws",data.toString())
  const response = JSON.parse(data.toString())
  console.log("id",response.msg)
  // const id = response.projectId;

  // const context =ConvHistory.get(id)
  // const resp = agent.invoke(context);

  // wss.send(resp)
})

})

app.listen(8080,()=>{
  console.log("server started to listen")
});
