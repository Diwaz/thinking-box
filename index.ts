import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { tool } from "@langchain/core/tools";
import * as z from "zod";
import { StateGraph, START, END, Command } from "@langchain/langgraph";
import { MessagesZodMeta } from "@langchain/langgraph";
import { registry } from "@langchain/langgraph/zod";
import { type BaseMessage } from "@langchain/core/messages";
import { isAIMessage, ToolMessage } from "@langchain/core/messages";
import { SystemMessage } from "@langchain/core/messages";
import { HumanMessage } from "@langchain/core/messages";
import { $ } from "bun";
import {exec} from "child_process"
import { applyPatch, createPatch } from "diff";


const llm = new ChatGoogleGenerativeAI({
  model: "gemini-2.5-flash",
});


const createfile = tool(
  async ({ filePath, content }) => {
    const file = Bun.file(filePath);
    await Bun.write(file, content);
    return `File created successfully at ${filePath}`
  }, {
  name: "creates_new_file",
  description: "Creates new file and adds content to it",
  schema: z.object({
    filePath: z.string().describe("File path of the origin of the file"),
    content: z.string().describe("content or code to put inside file")
  })
})

const runShellCommands = tool (
  async ({ command }) => {
    console.log("commdn",command);
    exec(`${command}`, (error, stdout, stderr) => {
    if (error) {
        console.log(`error: ${error.message}`);
        return;
    }
    if (stderr) {
        console.log(`stderr: ${stderr}`);
        return;
    }
    console.log(`stdout: ${stdout}`);
});

    return ` command executed in the terminal successfully `;
  },{
    name : "run_shell_command",
    description:"runs the shell command given by AI in the terminal",
    schema: z.object ({
      command: z.string().describe("shell command to run in bash terminal")
    })
  }
)
const applyPatchTool = tool (
  async ({file_to_edit,new_file})=>{
    console.log("applying patch")
    // apply patch logic 
    console.log("file path of the old file",file_to_edit)
    const oldFile = Bun.file(file_to_edit)
    const oldFilesContents = await oldFile.text();
    const patch = createPatch(file_to_edit,oldFilesContents,new_file)
    const patchedFile = applyPatch(oldFilesContents,patch)
    console.log("after applying patch",patchedFile)
    await Bun.write(file_to_edit,patchedFile)
  },
{
  name: "apply_patch_tool",
  description: "takes the file path of the old file to edit and new content to apply patch and updates the old file with updated content",
  schema: z.object({
    file_to_edit: z.string().describe("file path of the old file to edit new update generated by AI"),
    new_file: z.string().describe("new updated latest file given by the AI"),
  })
}
)
const toolsByName = {
  [createfile.name]: createfile,
  [runShellCommands.name]: runShellCommands,
  [applyPatchTool.name]:applyPatchTool
};

const tools = Object.values(toolsByName);
const llmWithTools = llm.bindTools(tools);


const MessageState = z.object({
  messages: z.array(z.custom<BaseMessage>()).register(registry, MessagesZodMeta),
  llmCalls: z.number().optional()
})

type State = z.infer<typeof MessageState>;

async function llmCall(state: State) {

  const llmResponse = await llmWithTools.invoke([
    new SystemMessage(
      "You are a helpful ai assistant that has wide experience in building web apps help user create robust and clean web app, ** remember always create a folder inside ai-projects folder which is inside root folder so do mkdir with prefix ai-projects/project-ID some random id then only create file inside that ** ** AND ALWAYS GIVE BACK THE LOCALHOST LINK TO THE USER IN RESPONSE AFTER RUNNING THE SHELL COMMAND HOSTING TO THE USER AND ALWAYS USE python3 -m http.server {PORT} always use python3 http-server since the projects are in plain html css and js **  **YOU ALWAYS WORK ON SAME EXISTING  PROJECT IF USER REQUEST YOU TO CREATE NEW PROJECT DENY THEIR REQUEST AND THERE ARE TOOLS WHICH WILL HELP YOU EDIT THE ALREADY EXISTING TOOLS"  ),
    ...state.messages
  ])

  return {
    messages: [...state.messages, llmResponse],
    llmCall: (state.llmCalls ?? 0) + 1,
  }

}

async function toolNode(state: State) {
  const lastMessage = state.messages.at(-1);

  if (lastMessage == null || !isAIMessage(lastMessage)) {
    return {
      messages: [],
    }
  }

  const result: ToolMessage[] = [];
  for (const toolCall of lastMessage.tool_calls ?? []) {
    const tool = toolsByName[toolCall.name];
    if (!tool) continue;
    const observation = await tool.invoke(toolCall);
    result.push(
      new ToolMessage({
        tool_call_id: toolCall.id,
        content:observation
      })
    );

  }

  return {
    messages: result
  }

}
async function shouldContinue(state: State) {
  const lastMessage = state.messages.at(-1);
  if (lastMessage == null || !isAIMessage(lastMessage)) {
    return END
  }

  if (lastMessage.tool_calls?.length) {
    return "toolNode";
  }
  return END;
}

const agent = new StateGraph(MessageState)
  .addNode("llmCall", llmCall)
  .addNode("toolNode", toolNode)
  .addEdge(START, "llmCall")
  .addConditionalEdges("llmCall", shouldContinue, ["toolNode", END])
  .addEdge("toolNode", "llmCall")
  .compile();

const start_agent = async () => {
  let state:State = {
    messages: []
  }
  while (true) {
    let inputFromUser = prompt("[You]:")

    if (inputFromUser?.toLowerCase() === "quit"){
      break;
    }
    if (!inputFromUser?.trim()){
      continue
    }
    state.messages.push(new HumanMessage(inputFromUser))
    const result = await agent.invoke(state)
    await Bun.write("output.txt",JSON.stringify(result.toString()));
    state = result

    for (const messages of state.messages) {
      // console.log("state messages",state.messages)
      console.log(`[${messages._getType()}]:${JSON.stringify(messages.content)}`);

    }




  }



}
start_agent();
